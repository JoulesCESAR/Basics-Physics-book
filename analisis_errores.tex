\chapter{Análisis de errores en la medición}

En toda actividad técnica y científica se realizan todo tipo de mediciones de las diferentes magnitudes y estás siempre 
presentan errores, éstos son inevitables en cualquier tipo de medición. Es así, que la realización de un análisis de estos 
errores son muy necesarios para evaluar las certezas toda actividad de medición. Por ejemplo en el campo de la ingeniería una 
falla en el análisis de errores de una medición puede traer como consecuencia accidentes increíbles, por otro lado 
 en las ciencias básicas tales como la Física, el proceso de medición y el análisis de errores poseen una importancia muy 
marcada, pues están relacionados íntimamente con el método científico.\\

En el método científico se describen de una u otra forma muchos fenómenos de la naturaleza a través de modelos matemáticos 
simple o complejos, donde surge la necesidad de analizar aquellos  modelos ya sea de forma analítica, con lápiz y papel, o a 
través de simulaciones numéricas, y tratando de encontrar cuáles son sus consecuencias o predicciones. Una vez obtenido este 
análisis, se compara con experimentos y observaciones donde las mediciones están presentes. Y por tanto en este proceso lo que se 
busca hallar acuerdos entre las predicciones de los modelos y lo observado, y para ello resulta inevitable realizar un análisis 
riguroso de los errores de las mediciones para establecer concordancias coherentes y veraces, de tal modo que se obtenga como 
consecuencia conclusiones veraces y reales.

\section{El proceso de medición de errores}

En el proceso de medición siempre existe un resultado, el cual es afectado por distintos errores que surgen de la interacción 
entre el aparato de medida, el observador y el sistema bajo estudio.\\ 

Así, los errores asociados a las mediciones pueden dividirse en dos grandes clases: a) \textbf{errores sistemáticos}, y b) 
\textbf{errores aleatorios}.\\


\subsection{Errores sistemáticos:}

Los errores sistemáticos se cometen de una misma manera cada vez que se mide.  Pueden estar originados en los defectos de los 
instrumentos de medida, en una particularidad del operador o del proceso de medición, etc. Estos errores son llamados también 
errores corregibles o determinados, a fines de distinguirlos de los errores aleatorios, los cuales se encuentran en toda medición 
y están fuera del control del observador. Los errores sistemáticos no se manifiestan como fluctuaciones aleatorias en los 
resultados de las mediciones. Por lo tanto, dado que el mismo error está involucrado en cada medición, no pueden eliminarse 
simplemente repitiendo las mediciones varias veces. En consecuencia, estos errores pueden eliminarse sólo después de realizar 
cuidadosas calibraciones y análisis de todas las posibles correcciones. 

\subsection{Errores aleatorios:}

Los errores aleatorios o accidentales, aparecen como fluctuaciones al azar en los valores de mediciones consecutivas. Estas 
variaciones aleatorias se deben a pequeños errores que escapan al control del observador. Por ejemplo, 
si se observa varias veces la presión indicada por la escala de un barómetro, los valores fluctuarán alrededor de un valor medio. 
De hecho de manera estricta se puede afirmar que no se puede obtener el valor verdadero de ninguna cantidad, sino sólo una 
aproximación. El propósito del tratamiento de los datos experimentales es justamente determinar el valor más probable de una 
cantidad medida y estimar su confiabilidad.

 \section{Origen de los errores}
 
Independientemente  de  la  naturaleza  de  los  errores,  estos  pueden  deberse  a  causas  que pueden clasificarse de la 
siguiente manera:

\subsection{Errores  debidos  al  observador:} 

Son los que se atribuyen a un defecto en las percepciones sensoriales del 
observador (como por ejemplo mala visión) o a la posición incorrecta del mismo para observar la experiencia.

\subsection{Errores  debidos  al  instrumento:} 

Estos errores dependen del instrumento utilizado y pueden dividirse en: 

\begin{itemize}
 \item[a)] Defecto de construcción de escala o un corrimiento permanente de la misma: se corrigen con una correcta calibración.
 
 \item[b)] Deficiencias de construcción o desgastes: estos errores los poseen todos los instrumentos y son muy difíciles de  
detectar (se  pueden  acotar  con  un  correcto mantenimiento del aparato).
 
 \item[c)] Limitaciones propias del sistema de lectura: este tipo de error se entiende mejor con ejemplos: el grosor de la aguja 
 indicadora o el espesor de la línea de división de la escala en un instrumento analógico.
 \end{itemize}


\subsection{Errores debido al modelo físico  elegido:} 

Estos errores provienen  de  las  aproximaciones realizadas al modelar la realidad con fundamentos teóricos. Por ejemplo, para  
calcular el período de un péndulo se asume que este es puntual, el hilo es de masa despreciable y los ángulos pequeños.

\subsection{Errores causados por el propio acto de medición:} 

Estos errores se deben a que todas las veces que un experimentador hace una observación altera el fenómeno que esta estudiando. 
Por ejemplo, cuando se mide la presión de un neumático con un manómetro, se libera algo de aire alterando la presión a medir.  

\subsection{Errores producidos por condiciones externas al proceso de medición:}  

Este tipo de  errores  se deben a las condiciones ambientales en las cuales se realiza una experiencia.  Son, en general, 
calculables en forma de correcciones para cada instrumento y para cada método de medida.  


\section{Precisión y exactitud}

Es costumbre generalizada, sobre todo en algunas normas relativas a instrumentos de medida, designar a la exactitud como la 
precisión de los mismos pero, tienen significados muy diferentes. \textbf{La exactitud} da una idea del grado de aproximación con 
que el valor medido concuerda con el valor  verdadero;  es decir, es la cercanía del valor experimental obtenido respecto al 
valor real de dicha medida. Por otro lado, \textbf{La precisión} se refiere a la repetibilidad de los resultados; es decir, el  
grado con el cual las medidas sucesivas arrojan idénticos valores. También está asociada a la sensibilidad o menor variación 
de la magnitud que se pueda detectar con un instrumento (o un método de medición).\\

Existen dos maneras de cuantificar el error de medición:\\

Mediante el llamado \textbf{error absoluto}, que corresponde a la diferencia entre el valor medido $X_m$ y el valor real $X_r$:

\begin{equation}
 E_{abs} = |X_m - X_r |
\end{equation}

Ó mediante  el  llamado \textbf{error  relativo},  que  corresponde  a  el  cociente  entre  el  error  absoluto  y  el valor 
real.

\begin{equation}
 E_{rel} =  E/X_r
 \end{equation}

\subsection{Resultado de una medición:}

El resultado de cualquier proceso de medición se compone del valor medido, de un símbolo que representa la unidad y del error que 
indica la ``exactitud'' con que se conoce el valor medido.  Con lo cual, el resultado de una medición queda expresado de la 
siguiente forma:

\begin{equation}
 X = (X_m \pm E_{abs})[\text{unidad de medida}]
\end{equation}

donde $X$ es la  magnitud que se desea medir o conocer; $X_m$  es  el  valor  medido  (representa  el número de veces que 
contiene a la unidad seleccionada); $E_{abs}$ es el error absoluto o incerteza (indica la exactitud con que se conoce el valor 
medido). Entonces, por medirse se entiende conocer el valor de una magnitud y conocer también el error con que se la mide en la 
unidad seleccionada.

\subsection{Intervalo de incerteza:}

Se dice que hay concordancia entre las predicciones teóricas de una hipótesis, modelo o teoría y los resultados de una medición,  
cuando  ambos  valores  coinciden  dentro  de  un  rango  definido  por  el error de medición. El error ``E'' define alrededor 
del valor medido un intervalo de incerteza igual al doble  del  error  (2E). Es otras palabras, indica una zona (intervalo) 
dentro de la cual está comprendido el verdadero valor de la magnitud: $(x_m - E, X_m + E)$.

\section{Cifras Significativas (c.s.)}

Los científicos e ingenieros procuran que sus datos experimentales no digan más de lo que pueden decir  (``asegurar'')  según  
las condiciones de medida en que fueron obtenidos. Por ello, ponen cuidado en el número de cifras con que expresan el resultado  
de una medición. El  propósito  de ello  es  incluir sólo aquellas  que  tienen  algún  significado  experimental. Tales cifras  
reciben  el nombre de cifras significativas.  Una cifra es significativa cuando se la conoce con una exactitud aceptable. Así,  
cuando  se  mide  con  un  termómetro  que  aprecia  hasta  $0,1^\circ C$  no  tiene  ningún sentido que se escriban resultados, 
por ejemplo, del tipo $36,25^\circ C$ o $22,175^\circ C$.\\

Esto es, la cantidad de  decimales  después  de  la  coma  esta  
relacionada  con  la  exactitud  del  instrumento  (y  no  con  la cantidad de dígitos que maneja una calculadora). Las cifras 
significativas no tienen ninguna relación \textit{fija} con la posición de la coma decimal; esto es, no tiene siempre que ser 1 o 
2 lugares.  La cantidad de decimales depende del instrumento utilizado para medir.\\


Una posible fuente de ambigüedad se presenta con el número de cifras significativas cuando se hace  un  cambio  de  unidades. El  
número  de  cifras  significativas  de  un  resultado  es  el  mismo, cualquiera que sea la unidad en la que se lo exprese. Dada 
una cantidad, la pregunta es ¿cuáles son cifras significativas?: a)Los  ceros  a  la  izquierda no  son  c.s.: Cuando  los  ceros  
figuran  como  primeras  cifras  de  un resultado  no  son  considerados  como  cifras  significativas. No  indican  exactitud  en 
 el resultado de la medición sino que indican el orden de magnitud de la unidad que acompaña al mismo, y b) Los  ceros  a  la  
derecha: Cuando  los  ceros  figuran  como  últimas  cifras  de  números  enteros, ello no implica que deban ser considerados 
necesariamente como cifras significativas.

\subsection{Empleo de cifras significativas:}


\subsubsection{Cifras significativas en operaciones aritméticas:}

Cuando se dispone de una calculadora electrónica parece como si fuese ``correcto'' o ``más exacto'' escribir los resultados con 
tantas cifras decimales como aparecen en pantalla, pero esto la mayoría de las veces carece de sentido. Para expresar 
correctamente los resultados de operaciones aritméticas, mediante cifras significativas, es necesario tener en cuenta que dicho 
resultado no puede tener más decimales que el número de menor cantidad de decimales involucrado en la operación.

\subsubsection{Reglas de aproximación o acotación de números:} 

Cuando  se  requiere  acotar  la  parte  decimal  de  un  número  hay  que  fijarse  en  el  número  de  su derecha. Si  éste  es 
 mayor o igual que  5,  entonces  se  redondea  incrementando  el  “último”  dígito significativo en +1.  Si es menor a 5, el 
``último'' dígito significativo permanece sin cambio; es decir, no se modifica. (éste método no es general pero es el que 
utilizaremos en este libro)

\section{Teoría Estadística de errores:}


En este apartado analizaremos los errores en la medición de una magnitud que se repite N veces.  Dado el carácter al azar de los 
errores es claro que, al promediar  los  resultados,  el  promedio  estará  menos  afectado  por  las  desviaciones estadísticas 
 que los valores individuales.  Se asume que no se cometen errores groseros y que los sistemáticos han sido debidamente acotados 
de manera tal que, los únicos errores a considerar sean los casuales. Para  analizar  la  serie  de N  mediciones  de  una  misma 
 magnitud  obtenida  en  igualdad  de condiciones  se  emplea  la  \textbf{Teoría  Estadística}.\\
 
La teoría estadística se basa en los tres postulados de Gauss:\\

i) Dada una serie de mediciones $x_1$, $x_2$,$\ldots$, $x_N$, la 
mejor estimación de la magnitud medida o valor más probable de la misma es el promedio aritmético de todas las mediciones de esa 
cantidad efectuadas en las mismas condiciones:

\begin{equation}
 \overline{x} = \frac{\sum_{i = 1}^{N} x_i}{N} = \frac{x_1+x_2+\ldots + x_N}{N}
\end{equation}

ii) Es igualmente probable cometer errores del mismo valor numérico y distinto signo.\\

iii) En  una  serie  de  mediciones, es tanto más probable un error cuanto menor sea su valorabsoluto. Es decir, los errores más 
pequeños son los más probables de cometer.\\

Se dice que la calidad de una medición será tanto mejor cuanto más parecidos sean entre sí los valores medidos, o dicho de otra 
forma, más parecidos al valor medio $\overline{x}$. Otros conceptos útiles en el análisis de una serie de mediciones son la 
mediana y la moda. La mediana hace énfasis en el verdadero ``centro'' del conjunto de datos. En otras palabras, la mediana es el 
valor central de un conjunto de mediciones ordenado por magnitud creciente o decreciente.  El  propósito  de  la  misma  es  
reflejar  la  tendencia  central  de  la  serie  de  medidas  de  manera  que  no esté  influenciada  por  los  valores  
extremos. Mientras  que  la moda (M)  es  aquel  valor  que  ocurre más  a  menudo  o  con  mayor  frecuencia. La moda puede 
no existir, y cuando existe no necesariamente es única.
 
\subsection{Error estadístico de la serie de N medidas:}

Dada  una  serie  de N  mediciones  de  la  magnitud $x$,  se  define  en  primer  lugar la desviación  de  la medición $epsilon_i$,  
la  cual  se  mide  respecto  del  valor  medio $x$  y  no  es  más  que  la  diferencia  existente entre  el  valor i-ésimo  
medido y el  valor  más  probable  (o  valor  medio  o  promedio  aritmético  de  la serie, i.e. $\overline{x}$): 

\begin{equation}
\epsilon_{i} = x - x_{i} = f_{i}( x - x_{i} ) 
\end{equation}

siendo, de nuevo, $f_i$ las veces que el i-ésimo valor $x_i$ se repite. La  sumatoria  de  la  desviación  ($\sum_i \epsilon_i$)  
no  tiene  significado  físico  e  incluso  puede  ser  cero;  en cambio, sí lo tiene la sumatoria de las desviaciones al 
cuadrado ($\sum_i \epsilon_i^2$) que representa la forma en que los valores individuales fluctúan alrededor del promedio.  Pero 
esta última cantidad depende de N.  Para independizarse de N es que se define la \textbf{varianza} $\nu$ como el promedio de las 
desviaciones cuadráticas:

\begin{equation}
 \nu = \frac{\sum_i \epsilon_i^2}{N}
\end{equation}

Es más común utilizar la raíz cuadrada de la varianza ($\sqrt{\nu}$) que proporciona la distribución de las mediciones alrededor 
del valor más probable ($\overline{x}$) pero con la misma unidad que los datos originales.  Dicha  cantidad  se  denomina  la 
dispersión  ó desviación  estándar  ó error  cuadrático  medio ($\sigma$)  de cada medida:

\begin{equation}
 \sigma = \sqrt{\nu} =\sqrt{\frac{\sum_i \epsilon_i^2}{N}}
\end{equation}

La desviación estándar es una medida del grado de dispersión de los datos alrededor del valor promedio.  Dicho de otra manera, la 
desviación estándar es simplemente el ``promedio'' o variación esperada con respecto de la media aritmética.  Una desviación 
estándar grande indica que los puntos están lejos de la media, y una desviación pequeña indica que los datos están agrupados 
cercanos a la media.\\

Un parámetro que nos  da  el  orden  de  magnitud  con  el  cuál  el  promedio  habrá  de  fluctuar  alrededor  del 
``verdadero  valor''  de  la  magnitud  en  cuestión  y  se  mantendrá  casi  constante  cuando  el  número  de observaciones es 
suficientemente grande (N grande) es el error estadístico:

\begin{equation}
 E_{est}=\frac{\sigma}{\sqrt{N}}
\end{equation}

, cuanto más mediciones se realicen, tanto más se acercará el promedio al ``verdadero valor'' de la magnitud en cuestión, y la 
fluctuación será cada vez menor.  Es  por  ello  que  el  promedio  es  utilizado  como  ente  representativo  del  valor  más  
probable  de  una magnitud.  

Hay que tomar en cuenta lo siguiente para nuestros procesos de medición:\\

\textit{Como no tiene sentido disminuir $E_{est}$ más allá de la apreciación $\Delta x$ del instrumento de medición, resulta más  
correcto cambiar el instrumento o mejorar el método que aumentar el número de mediciones. Es preferible obtener veinte ``buenas'' 
medidas y no mil mediocres.}\\

\subsection{Error medio cuadrático (rms)}

En Física experimental resulta muchas veces evaluar el error cuadrática metido para una colección de N valores 
{$x_1, x_2, \ldots , x_N$} de una variable discreta $x$, el cual dado por:


\begin{equation}
 x_{rms} =\sqrt{  \frac{1}{N}\sum_{i = 1}^N x_{i}^2 }
\end{equation}

\vspace{1.5cm}

Con todo lo anterior analizado es posible dar como medida final de las $x_i$ mediciones como:

\begin{equation}
 x =  \overline{x} \pm \sigma
\end{equation}









 
 
